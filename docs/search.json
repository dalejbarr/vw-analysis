[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Analysing Visual-World Eyetracking Data Reproducibly",
    "section": "",
    "text": "For this workshop, we will be reproducing data from a study by Weighall et al. (2017) on the role of sleep in learning novel words (lexical consolidation).\nWeighall, A. R., Henderson, L. M., Barr, D. J., Cairney, S. A., & Gaskell, M. G. (2017). Eye-tracking the time‐course of novel word learning and lexical competition in adults and children. Brain and Language, 167, 13-27.\n\n\n\nFigure ?fig-weighall shows the data.\n\n\n\n\nWeighall, AR, Lisa-Marie Henderson, DJ Barr, Scott Ashley Cairney, and Mark Gareth Gaskell. 2017. “Eye-Tracking the Time-Course of Novel Word Learning and Lexical Competition in Adults and Children.” Brain and Language 167: 13–27."
  },
  {
    "objectID": "preprocessing_1.html",
    "href": "preprocessing_1.html",
    "title": "1  Import, epoching, and time-alignment",
    "section": "",
    "text": "The overall task here is to scrape out the data we want to use from each trial (epoching) and align the frame counters for all trials to the disambiguation point for the particular audio stimulus that was played on that trial (time-alignment). In other words, the disambiguation point should be the temporal “origin” (zero point) for the timeline on each trial."
  },
  {
    "objectID": "preprocessing_1.html#data-import",
    "href": "preprocessing_1.html#data-import",
    "title": "1  Import, epoching, and time-alignment",
    "section": "1.1 Data import",
    "text": "1.1 Data import\nFor the first part of pre-processing, we will load the eye data into our R session using functions from the {readr} package, which is one of many packages that is part of the {tidyverse} meta-package. The .gazedata files from the Tobii eyetracking system are in .tsv or Tab Separated Values format, for which we use read_tsv().\nBefore we can perform epoching and time-alignment, we have to import and clean up the .gazedata files. These are 42 adult data files and 41 child data files located in the adult and child subdirectories of data-raw/. These files follow the naming convention data-raw/adult/sub_XXX.gazedata and data-raw/child/sub_XXX.gazedata where the XXX part of the filename the unique integer identifying each subject, which corresponds to sub_id in the subjects table.\nThe raw gazedata files include a lot of unnecessary information. We’ll need to scrape out the data that we need and convert the XXX value from the filename into a sub_id variable in the resulting table. The source files have the format below.\n\n\n\n\n\n\n\n\n\n\nvariable\ntype\ndescription\n\n\n\n\nID\n\narbitrary value uniquely identifying each frame within subject\n\n\nTETTime\n\n(ignored)\n\n\nRTTime\n\n(ignored)\n\n\nCursorX\n\nhorizontal point of gaze in pixels\n\n\nCursorY\n\nvertical point of gaze in pixels\n\n\nTimestampSec\n\ntimestamp in seconds\n\n\nTimestampMicrosec\n\nmillisecond portion of timestamp (cycles around)\n\n\nXGazePosLeftEye\n\n(ignored)\n\n\nYGazePosLeftEye\n\n(ignored)\n\n\nXCameraPosLeftEye\n\n(ignored)\n\n\nYCameraPosLeftEye\n\n(ignored)\n\n\nDiameterPupilLeftEye\n\n(ignored)\n\n\nDistanceLeftEye\n\n(ignored)\n\n\nValidityLeftEye\n\n(ignored)\n\n\nXGazePosRightEye\n\n(ignored)\n\n\nYGazePosRightEye\n\n(ignored)\n\n\nXCameraPosRightEye\n\n(ignored)\n\n\nYCameraPosRightEye\n\n(ignored)\n\n\nDiameterPupilRightEye\n\n(ignored)\n\n\nDistanceRightEye\n\n(ignored)\n\n\nValidityRightEye\n\n(ignored)\n\n\nTrialId\n\narbitrary value uniquely identifying each trial within a subject (same as t_id)\n\n\nUserDefined_1\n\nphase of the trial (Fixation, Preview, StimSlide)\n\n\n\n\n\n\n1.1.1 Activity: One Subject\nRead in the Tobii eyetracking data for a single subject from the datafile data-raw/adult/sub_003.gazedata, and convert it to the format below.\n\n\n# A tibble: 16,658 × 7\n   sub_id  t_id  f_id         sec     x     y phase  \n    <int> <int> <int>       <dbl> <int> <int> <chr>  \n 1      3     1   145 1317141127.   666   521 Preview\n 2      3     1   146 1317141127.   649   442 Preview\n 3      3     1   147 1317141127.   618   507 Preview\n 4      3     1   148 1317141127.   645   471 Preview\n 5      3     1   149 1317141127.   632   471 Preview\n 6      3     1   150 1317141127.   645   536 Preview\n 7      3     1   151 1317141127.   651   474 Preview\n 8      3     1   152 1317141127.   643   541 Preview\n 9      3     1   153 1317141127.   628   581 Preview\n10      3     1   154 1317141127.   643   532 Preview\n# … with 16,648 more rows\n\n\nHere, we have renamed TrialId to t_id, which is the name it takes throughout the rest of the database. We have also renamed CursorX and CursorY to x and y respectively. We have also renamed ID to f_id (frame id) and UserDefined_1 to phase. We also exclude any frames from the phase where UserDefined_1 == \"Fixation\", because these frames are not informative, and doing so reduces the size of the data we need to import.\n\n\n\n\n\n\nHint: Importing only those columns you need\n\n\n\n\n\nUse the col_types argument to read_tsv() and the cols_only() specification.\nFor instance, something like:\n\nread_tsv(\"data-raw/adult/sub_003.gazedata\",\n         col_types = cols_only(ID = col_integer(),\n                              # [..etc]\n                              ),\n         #.. other args to read_tsv,\n         )\n\nType ?readr::cols_only in the console to learn more about specifying columns during data import.\n\n\n\n\n\n\n\n\n\nHint: Extracting the subject id number\n\n\n\n\n\nYou can use the id argument to read_tsv() to specify the name of a variable in the resulting data frame that has the filename as its value.\nYou can then create a new variable using mutate() that extracts the XXX substring (positions 20-22 of the string) and then converts it to an integer.\n\nread_tsv(\"data-raw/adult/sub_003.gazedata\",\n         id = \"filename\",\n         # other args to read_tsv()...\n         ) %>%\n  mutate(sub_id = substr(filename, 20, 22) %>% as.integer()) # %>%\n  ## rest of your pipeline..\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nlibrary(\"tidyverse\")\n\n## make sure that your working directory is properly set!\n\nread_tsv(\"data-raw/adult/sub_003.gazedata\",\n         col_types = cols_only(ID = col_integer(),\n                               TrialId = col_integer(),\n                               CursorX = col_integer(),\n                               CursorY = col_integer(),\n                               TimestampSec = col_integer(),\n                               TimestampMicrosec = col_integer(),\n                               UserDefined_1 = col_character()),\n         id = \"filename\") %>%\n  ## convert XXX to sub_id\n  mutate(sub_id = substr(filename, 20, 22) %>% as.integer(),\n         sec = TimestampSec + TimestampMicrosec / 1000000) %>%\n  select(sub_id, t_id = TrialId, f_id = ID,\n         sec, x = CursorX, y = CursorY,\n         phase = UserDefined_1) %>%\n  filter(phase != \"Fixation\")\n\n\n\n\n\n\n1.1.2 Activity: All Subjects\nNow adapt the code that you wrote above to load in all 83 into a single table, which should have the same format as for the data you imported for subject 3 above.\n\n\n\n\n\n\nTip\n\n\n\n\n\nThe readr functions like read_tsv() make it easy to read in multiple files. All you need to do is to provide a vector of filenames as the first argument.\nFor example, read_tsv(c(\"file1.tsv\", \"file2.tsv\")) will read both file1.tsv and file2.tsv and bind together the rows imported from both files in the result.\n\n\n\n\n\n\n\n\n\nHint: How do I get a vector of all the files in a directory?\n\n\n\n\n\nThe dir() function for base R can be used to list files. Examples:\n\ndir(\"data-raw\")\n\n[1] \"adult\"              \"child\"              \"locations.csv\"     \n[4] \"screens.csv\"        \"speech-timings.csv\" \"stimuli.csv\"       \n[7] \"subjects.csv\"       \"trials.csv\"        \n\n\n\nadults <- dir(\"data-raw/adult\", full.names = TRUE)\n\nadults\n\n [1] \"data-raw/adult/sub_001.gazedata\" \"data-raw/adult/sub_002.gazedata\"\n [3] \"data-raw/adult/sub_003.gazedata\" \"data-raw/adult/sub_004.gazedata\"\n [5] \"data-raw/adult/sub_005.gazedata\" \"data-raw/adult/sub_006.gazedata\"\n [7] \"data-raw/adult/sub_007.gazedata\" \"data-raw/adult/sub_008.gazedata\"\n [9] \"data-raw/adult/sub_009.gazedata\" \"data-raw/adult/sub_010.gazedata\"\n[11] \"data-raw/adult/sub_011.gazedata\" \"data-raw/adult/sub_012.gazedata\"\n[13] \"data-raw/adult/sub_013.gazedata\" \"data-raw/adult/sub_014.gazedata\"\n[15] \"data-raw/adult/sub_015.gazedata\" \"data-raw/adult/sub_016.gazedata\"\n[17] \"data-raw/adult/sub_017.gazedata\" \"data-raw/adult/sub_018.gazedata\"\n[19] \"data-raw/adult/sub_019.gazedata\" \"data-raw/adult/sub_020.gazedata\"\n[21] \"data-raw/adult/sub_021.gazedata\" \"data-raw/adult/sub_022.gazedata\"\n[23] \"data-raw/adult/sub_023.gazedata\" \"data-raw/adult/sub_024.gazedata\"\n[25] \"data-raw/adult/sub_025.gazedata\" \"data-raw/adult/sub_026.gazedata\"\n[27] \"data-raw/adult/sub_027.gazedata\" \"data-raw/adult/sub_028.gazedata\"\n[29] \"data-raw/adult/sub_029.gazedata\" \"data-raw/adult/sub_030.gazedata\"\n[31] \"data-raw/adult/sub_031.gazedata\" \"data-raw/adult/sub_032.gazedata\"\n[33] \"data-raw/adult/sub_033.gazedata\" \"data-raw/adult/sub_034.gazedata\"\n[35] \"data-raw/adult/sub_035.gazedata\" \"data-raw/adult/sub_036.gazedata\"\n[37] \"data-raw/adult/sub_037.gazedata\" \"data-raw/adult/sub_039.gazedata\"\n[39] \"data-raw/adult/sub_040.gazedata\" \"data-raw/adult/sub_041.gazedata\"\n[41] \"data-raw/adult/sub_042.gazedata\" \"data-raw/adult/sub_043.gazedata\"\n\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n## get .gazedata filenames\nadults <- dir(\"data-raw/adult\", full.names = TRUE)\nchildren <- dir(\"data-raw/child\", full.names = TRUE)\n\nedat <- read_tsv(c(adults, children),\n                 col_types = cols_only(ID = col_integer(),\n                                       TrialId = col_integer(),\n                                       CursorX = col_integer(),\n                                       CursorY = col_integer(),\n                                       TimestampSec = col_integer(),\n                                       TimestampMicrosec = col_integer(),\n                                       UserDefined_1 = col_character()),\n                 id = \"filename\") %>%\n  mutate(sub_id = substr(filename, 20, 22) %>% as.integer(),\n         sec = TimestampSec + TimestampMicrosec / 1000000) %>%\n  select(sub_id, t_id = TrialId, f_id = ID,\n         sec, x = CursorX, y = CursorY,\n         phase = UserDefined_1) %>%\n  filter(phase != \"Fixation\")\n\nedat\n\n# A tibble: 1,899,013 × 7\n   sub_id  t_id  f_id         sec     x     y phase  \n    <int> <int> <int>       <dbl> <int> <int> <chr>  \n 1      1     1   272 1317113393.   628   523 Preview\n 2      1     1   273 1317113393.   634   529 Preview\n 3      1     1   274 1317113393.   633   519 Preview\n 4      1     1   275 1317113393.   644   531 Preview\n 5      1     1   276 1317113393.   637   520 Preview\n 6      1     1   277 1317113393.   635   515 Preview\n 7      1     1   278 1317113393.   636   519 Preview\n 8      1     1   279 1317113393.   638   518 Preview\n 9      1     1   280 1317113393.   642   519 Preview\n10      1     1   281 1317113393.   638   518 Preview\n# … with 1,899,003 more rows"
  },
  {
    "objectID": "preprocessing_1.html#epoching-and-time-alignment",
    "href": "preprocessing_1.html#epoching-and-time-alignment",
    "title": "1  Import, epoching, and time-alignment",
    "section": "1.2 Epoching and time-alignment",
    "text": "1.2 Epoching and time-alignment\nThe Tobii eyetracker recorded data at a rate of 60 Hertz (i.e., 60 frames per second, or one frame every 1/60th of a second.) For each trial, the frame counter (ID, which we renamed to f_id) starts at 1 and increments every frame. This is not very useful because we need to know when certain stimulus events occurred, and these will take place at a different frame number for every trial, depending on the timing of the speech events of the stimulus for that trial. We need to re-define the ‘origin’ of the eye-tracking data. In this study, we used the ‘disambiguation point’, which is the point in the word where the signal distinguishes between two competing lexical items (e.g., candy and candle).\n\nAs ?fig-epoching shows, each trial had three phases, a Fixation, Preview, and StimSlide phase, which are indexed by the variable phase. Playback of a soundfile with a pre-recorded speech stimulus began simultaneously with the onset of the StimSlide phase.\nFor each trial (uniquely identified by sub_id and t_id), we are going to need to do two things to time-align the eye data to the disambiguation point.\n\nFind out what sound was played and the timing of the disambiguation point within that soundfile, as measured from the start of the file.\nFigure out the frame number corresponding to the start of the StimSlide phase and then adjust by the amount calculated in the previous step.\n\n\n1.2.1 Activity: Disambiguation Point\nCreate the table below from the raw data, which has information about the onset of the disambiguation point for each trial. Store the table as origin_adj.\nYou may wish to consult Appendix A to see what tables the values in the table below have been are drawn from. You’ll need to import these tables into your session. All of these tables have the extension .csv, which indicates they are in Comma Separated Values format. The ideal way to import these files is to use read_csv() from the {readr} package.\n\n\n# A tibble: 5,644 × 4\n   sub_id  t_id sound          disambig_point\n    <int> <int> <chr>                   <int>\n 1      1     1 Tpelican.wav             1171\n 2      1     2 Tpumpkin.wav             1079\n 3      1     3 pencil.wav                810\n 4      1     4 paddle.wav                881\n 5      1     6 Tbalcony.wav             1012\n 6      1     7 Tnapkin.wav              1069\n 7      1    11 Tflamingo.wav            1150\n 8      1    13 Tangel.wav               1036\n 9      1    14 Tparachute.wav           1046\n10      1    16 Tmushroom.wav            1062\n# … with 5,634 more rows\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\ntrials <- read_csv(\"data-raw/trials.csv\",\n                   col_types = \"iiiiii\")\n\nstimuli <- read_csv(\"data-raw/stimuli.csv\",\n                    col_types = \"iiciccc\")\n\nspeech <- read_csv(\"data-raw/speech-timings.csv\",\n                   col_types = \"ciii\")\n\norigin_adj <- trials %>%\n  inner_join(stimuli, \"iv_id\") %>% # to get `sound`\n  select(sub_id, t_id, sound) %>% \n  inner_join(speech, \"sound\") %>% # to get the timings\n  select(-article, -noun)\n\n\n\n\n\n\n1.2.2 Activity: Onset of StimSlide\nNow let’s do part 2, where we find the value of f_id for the first frame of eyedata for each trial following the onset of the StimSlide phase. We should have a table that looks like the one below, with one row for each trial, and where f_ss is the value of f_id for the earliest frame in the StimSlide phase.\n\n\n# A tibble: 7,385 × 3\n   sub_id  t_id  f_ss\n    <int> <int> <int>\n 1      1     1   338\n 2      1     2   729\n 3      1     3  1124\n 4      1     4  1443\n 5      1     5  1795\n 6      1     6  2300\n 7      1     7  2593\n 8      1     8  3348\n 9      1     9  3874\n10      1    10  4331\n# … with 7,375 more rows\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n## figure out the f_id for the earliest StimSlide frame\norigin_frames <- edat %>% \n  filter(phase == \"StimSlide\") %>%\n  group_by(sub_id, t_id) %>%\n  summarise(f_ss = min(f_id),\n            .groups = \"drop\")\n\norigin_frames\n\n\n\n\n\n\n1.2.3 Activity: Combine origins\nNow that we have the first frame of StimSlide and the adjustment we have to make in milliseconds for the disambiguation point, combine the tables and calculate f_z, which will represent the “zero points” in frames for each trial. Store the resulting table in origins.\n\n\n# A tibble: 5,643 × 5\n   sub_id  t_id  f_ss disambig_point   f_z\n    <int> <int> <int>          <int> <int>\n 1      1     1   338           1171   408\n 2      1     2   729           1079   794\n 3      1     3  1124            810  1173\n 4      1     4  1443            881  1496\n 5      1     6  2300           1012  2361\n 6      1     7  2593           1069  2657\n 7      1    11  4699           1150  4768\n 8      1    13  5395           1036  5457\n 9      1    14  5893           1046  5956\n10      1    16  6811           1062  6875\n# … with 5,633 more rows\n\n\n\n\n\n\n\n\nHint: How to convert milliseconds to frames of eye data\n\n\n\n\n\nThere are 60 frames per second, so 60 frames per 1000 milliseconds.\nSo to convert from milliseconds to frames:\nf_z = 60 * ms / 1000\nFor example, if you have 500 ms, then 60 * 500 / 1000 = 30.\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\norigins <- origin_frames %>%\n  inner_join(origin_adj, c(\"sub_id\", \"t_id\")) %>%\n  mutate(f_z = round(f_ss + 60 * disambig_point / 1000) %>%\n           as.integer()) %>%\n  select(-sound)\n\n\n\n\n\n\n1.2.4 Activity: Time-align\nNow we’re ready to calculate a new frame index on our eye data (edat), f_c, which is centered on the zero point, f_z. The resulting table should be called epdat and have the following structure.\n\n\n# A tibble: 1,341,405 × 7\n   sub_id  t_id  f_id   f_z   f_c     x     y\n    <int> <int> <int> <int> <int> <int> <int>\n 1      1     1   272   408  -136   628   523\n 2      1     1   273   408  -135   634   529\n 3      1     1   274   408  -134   633   519\n 4      1     1   275   408  -133   644   531\n 5      1     1   276   408  -132   637   520\n 6      1     1   277   408  -131   635   515\n 7      1     1   278   408  -130   636   519\n 8      1     1   279   408  -129   638   518\n 9      1     1   280   408  -128   642   519\n10      1     1   281   408  -127   638   518\n# … with 1,341,395 more rows\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nepdat <- edat %>%\n  inner_join(origins, c(\"sub_id\", \"t_id\")) %>%\n  mutate(f_c = f_id - f_z) %>%\n  select(sub_id, t_id, f_id, f_z, f_c, x, y)"
  },
  {
    "objectID": "preprocessing_1.html#save-the-data",
    "href": "preprocessing_1.html#save-the-data",
    "title": "1  Import, epoching, and time-alignment",
    "section": "1.3 Save the data",
    "text": "1.3 Save the data\nWe’ve reached a stopping point. We’ll want to save the epoched data so that we can use that as our starting point for the next preprocessing stage. We’ll remove the variables f_id and f_z because we no longer need them. We’ll also keep 1.5 seconds (90 frames) before and after the disambiguation point for each trial.\n\n## if we haven't made a \"data-derived\" directory, do so now\nif (!dir.exists(\"data-derived\")) dir.create(\"data-derived\")\n\nepdat %>%\n  filter(f_c >= -90L, f_c <= 90L) %>%\n  select(-f_id, -f_z) %>%\n  saveRDS(file = \"data-derived/edat-epoched.rds\")"
  },
  {
    "objectID": "preprocessing_2.html",
    "href": "preprocessing_2.html",
    "title": "2  Mapping gaze to areas of interest",
    "section": "",
    "text": "At this point we have epoched our eyetracking data, resulting in the edat-epoched.rds file which looks like so:\nWe know when people are looking relative to the disambiguation point for the trial (f_c), and we know where they are looking, because we have the (x, y) coordinates. But we yet don’t know which image they are looking at on each frame. So we have to map the two-dimensional gaze coordinates onto the coordinates of the images that was displayed on a given trial.\nWe know what pictures were shown on each trial from the data in the screens table (from data-raw/screens.csv).\nThe table looks like so.\nThe loc variable is a number that refers to the four quadrants of the screen where the images appeared. We can get the pixel coordinates representing the top left and bottom right corners of each rectangle from the locations table."
  },
  {
    "objectID": "preprocessing_2.html#image-locations-for-each-trial",
    "href": "preprocessing_2.html#image-locations-for-each-trial",
    "title": "2  Mapping gaze to areas of interest",
    "section": "2.1 Image locations for each trial",
    "text": "2.1 Image locations for each trial\n\n2.1.1 Activity: Get coordinates\nWe want to combine the data from screens and locations with trial info to create the following table, which we will use later to figure out what image was being looked at (if any) on each frame of each trial. Save this information in a table named aoi (for Area Of Interest). You might need to reference Appendix A to see how to get sub_id and t_id into the table.\n\n\n# A tibble: 22,576 × 8\n   sub_id  t_id  s_id role        x1    y1    x2    y2\n    <int> <int> <int> <chr>    <int> <int> <int> <int>\n 1      1     1   183 critical   704   564  1279  1023\n 2      1     1   183 existing     0   564   575  1023\n 3      1     1   183 novel        0     0   575   460\n 4      1     1   183 target     704     0  1279   460\n 5      1     2   194 critical     0   564   575  1023\n 6      1     2   194 existing   704     0  1279   460\n 7      1     2   194 novel        0     0   575   460\n 8      1     2   194 target     704   564  1279  1023\n 9      1     3    33 critical   704     0  1279   460\n10      1     3    33 existing     0   564   575  1023\n# … with 22,566 more rows\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nWe can get sub_id and t_id from trials. But to get there from screens, we need to get the item version (iv_id) from stimuli. We can connect screens to stimuli through the screen id (s_id).\n\ntrials <- read_csv(\"data-raw/trials.csv\",\n                   col_types = \"iiiiii\")\n\nstimuli <- read_csv(\"data-raw/stimuli.csv\",\n                    col_types = \"iiciccc\")\n\naoi <- trials %>%\n  select(sub_id, t_id, iv_id) %>%\n  inner_join(stimuli, \"iv_id\") %>%\n  inner_join(screens, \"s_id\") %>%\n  inner_join(locations, \"loc\") %>%\n  select(sub_id, t_id, s_id, role, x1, y1, x2, y2)\n\nAs a check, we should have four times the number of rows as trials (5644), because there should be four areas of interest for each trial. We can use stopifnot() to make our script terminate if this condition is not satisfied.\n\nstopifnot( nrow(aoi) == 4 * nrow(trials) )"
  },
  {
    "objectID": "preprocessing_2.html#identifying-frames-where-the-gaze-cursor-is-within-an-aoi",
    "href": "preprocessing_2.html#identifying-frames-where-the-gaze-cursor-is-within-an-aoi",
    "title": "2  Mapping gaze to areas of interest",
    "section": "2.2 Identifying frames where the gaze cursor is within an AOI",
    "text": "2.2 Identifying frames where the gaze cursor is within an AOI\nWhat we need to do now is look at the (x, y) coordinates in edat and see if they fall within the bounding box for each image in the aoi table for the corresponding trial.\n\n2.2.1 Activity: Create frames_in\nThere are different ways to accomplish this task, but an effective strategy is just to join the eyedata (edat) to the aoi table and retain any frames where the x coordinate of the eye gaze is within the x1 and x2 coordinates of the rectangle, and the y coordinate is within the y1 and y2 coordinates. Because our AOIs do not overlap, the gaze can only be within a single AOI at a time.\nName the resulting table frames_in.\n\n\n\n\n\n\nHint\n\n\n\n\n\nSome code to get you started.\n\nedat %>%\n  inner_join(aoi, c(\"sub_id\", \"t_id\")) # %>%\n  ## filter(...) \n\n\n\n\n\n\n# A tibble: 759,311 × 4\n   sub_id  t_id   f_c role  \n    <int> <int> <int> <chr> \n 1      1     1   -90 target\n 2      1     1   -89 target\n 3      1     1   -88 target\n 4      1     1   -87 target\n 5      1     1   -86 target\n 6      1     1   -85 target\n 7      1     1   -84 target\n 8      1     1   -83 target\n 9      1     1   -82 target\n10      1     1   -81 target\n# … with 759,301 more rows\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nframes_in <- edat %>%\n  inner_join(aoi, c(\"sub_id\", \"t_id\")) %>%\n  filter(x >= x1, x <= x2,\n         y >= y1, y <= y2) %>%\n  select(sub_id, t_id, f_c, role)\n\n\n\n\n\n\n2.2.2 Activity: Create frames_out\nCreate a table frames_out containing only those frames from edat where the gaze fell outside of any of the four image regions, and label those with the role (blank). Use the anti_join() function from dplyr to do so.\nThe resulting table should have the format below.\n\n\n# A tibble: 182,504 × 4\n   sub_id  t_id   f_c role   \n    <int> <int> <int> <chr>  \n 1      1     1   -69 (blank)\n 2      1     1   -68 (blank)\n 3      1     1   -66 (blank)\n 4      1     1   -49 (blank)\n 5      1     1    -9 (blank)\n 6      1     1    -8 (blank)\n 7      1     1    -7 (blank)\n 8      1     1    -6 (blank)\n 9      1     1    -5 (blank)\n10      1     1    -4 (blank)\n# … with 182,494 more rows\n\n\n\n\n\n\n\n\nHint: Show me an example of anti_join()\n\n\n\n\n\n\ntable_x <- tibble(letter = c(\"A\", \"B\", \"C\", \"D\", \"E\"),\n                  number = c(1, 2, 3, 4, 5))\n\ntable_x\n\n# A tibble: 5 × 2\n  letter number\n  <chr>   <dbl>\n1 A           1\n2 B           2\n3 C           3\n4 D           4\n5 E           5\n\ntable_y <- tibble(letter = c(\"C\", \"D\", \"E\"),\n                  number = c(3, 4, 99))\n\ntable_y\n\n# A tibble: 3 × 2\n  letter number\n  <chr>   <dbl>\n1 C           3\n2 D           4\n3 E          99\n\n## which rows in table_x are not in table_y?\nanti_join(table_x, table_y, c(\"letter\", \"number\"))\n\n# A tibble: 3 × 2\n  letter number\n  <chr>   <dbl>\n1 A           1\n2 B           2\n3 E           5\n\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nframes_out <- edat %>% \n  select(sub_id, t_id, f_c) %>%\n  anti_join(frames_in, c(\"sub_id\", \"t_id\", \"f_c\")) %>%\n  mutate(role = \"(blank)\")\n\nA good test to do at this point is to make sure that all 941,815 rows of edat have been assigned to either frames_in or frames_out.\n\nstopifnot( nrow(edat) == (nrow(frames_in) + nrow(frames_out)) ) # TRUE\n\n\n\n\n\n\n2.2.3 Activity: Combine into pog\nCombine frames_in and frames_out into a single table by concatenating the rows. Sort the rows so by sub_id, t_id, and f_c, and convert role into type factor with levels in this order: target, critical, existing, novel, and (blank). The resulting table should be called pog and have the format below.\n\n\n# A tibble: 941,815 × 4\n   sub_id  t_id   f_c role  \n    <int> <int> <int> <fct> \n 1      1     1   -90 target\n 2      1     1   -89 target\n 3      1     1   -88 target\n 4      1     1   -87 target\n 5      1     1   -86 target\n 6      1     1   -85 target\n 7      1     1   -84 target\n 8      1     1   -83 target\n 9      1     1   -82 target\n10      1     1   -81 target\n# … with 941,805 more rows\n\n\n\n\n\n\n\n\nHow do I concatenate two tables?\n\n\n\n\n\nUse the bind_rows() function from {dplyr}.\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\n\nWe might want to check that role has been defined properly.\n\npog %>%\n  pull(role) %>%\n  levels()\n\n[1] \"target\"   \"critical\" \"existing\" \"novel\"    \"(blank)\""
  },
  {
    "objectID": "preprocessing_2.html#dealing-with-trial-dropouts",
    "href": "preprocessing_2.html#dealing-with-trial-dropouts",
    "title": "2  Mapping gaze to areas of interest",
    "section": "2.3 Dealing with trial dropouts",
    "text": "2.3 Dealing with trial dropouts\n\n\n\nWe want to be able to use the data in pog to calculate probabilities of gazing at regions over time. However, we are not ready to do this yet.\nIf we look at the first seven trials from subject 3, we can see that there is a problem, because the trials end at different times, due to variation in response time. If we plot the resulting data, we will have fewer and fewer data points as we progress through the trial.\n\n\n\n\n\nA solution to this is to make each time series “cumulative to selection”, which means padding frames after the trial ends with artificial looks to the object that was selected. In other words, we pretend that the subject remained fixated on the selected object after clicking.\nBut before we do this, we should double check that trials also start at the same frame (-90). Once we pass this sanity check we can pad frames at the end.\n\nstart_frame <- edat %>% \n  group_by(sub_id, t_id) %>% \n  summarise(min_f_c = min(f_c), # get the minimum frame per trial\n            .groups = \"drop\") %>%\n  pull(min_f_c) %>%\n  unique() # what are the unique values?\n\n## if the value is the same for every trial, there should be\n## just one element in this vector\nstopifnot( length(start_frame) == 1L )\n\nstart_frame\n\n[1] -90\n\n\n\n2.3.1 Activity: Selected object\nWhich object was selected on each trial? The trials table tells us which location was clicked (1, 2, 3, 4) but not which object. We need to figure out which object was clicked by retrieving that information from the screens table. The result should have the format below.\n\n\n# A tibble: 5,644 × 3\n   sub_id  t_id role  \n    <int> <int> <chr> \n 1      1     1 target\n 2      1     2 target\n 3      1     3 target\n 4      1     4 target\n 5      1     6 target\n 6      1     7 target\n 7      1    11 target\n 8      1    13 target\n 9      1    14 target\n10      1    16 target\n# … with 5,634 more rows\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n## which object was selected on each trial?\nselections <- trials %>%\n  inner_join(stimuli, \"iv_id\") %>%\n  inner_join(screens, c(\"s_id\", \"resploc\" = \"loc\")) %>%\n  select(sub_id, t_id, role)\n\n\n\n\nNow that we know what object was selected, we want to pad trials up to the latest frame in the dataset, which we determined during epoching as frame 90 (that is, 1.5 seconds after the disambiguation point).\nWe will use the crossing() function (from {tidyr}) to create a table with all combinations of the rows from selections with frames f_c from 0 to 90. Then, in the next activity, we will use anti_join() to pull out the combinations that are missing from pog, and use them in padding.\n\nall_frames <- crossing(selections, tibble(f_c = 0:90))\n\nall_frames\n\n# A tibble: 513,604 × 4\n   sub_id  t_id role     f_c\n    <int> <int> <chr>  <int>\n 1      1     1 target     0\n 2      1     1 target     1\n 3      1     1 target     2\n 4      1     1 target     3\n 5      1     1 target     4\n 6      1     1 target     5\n 7      1     1 target     6\n 8      1     1 target     7\n 9      1     1 target     8\n10      1     1 target     9\n# … with 513,594 more rows\n\n\n\n\n2.3.2 Activity: Pad frames\nUse anti_join() to find out which frames in all_frames are missing from pog. Concatenate these frames onto pog, storing the result in pog_cts. The resulting table should have a variable pad which is FALSE if the frame is an original one, and TRUE if it was added through the padding procedure. Sort the rows of pog_cts by sub_id, t_id, and f_c. The format is shown below.\n\n\n# A tibble: 1,021,288 × 5\n   sub_id  t_id   f_c role   pad  \n    <int> <int> <int> <chr>  <lgl>\n 1      1     1   -90 target FALSE\n 2      1     1   -89 target FALSE\n 3      1     1   -88 target FALSE\n 4      1     1   -87 target FALSE\n 5      1     1   -86 target FALSE\n 6      1     1   -85 target FALSE\n 7      1     1   -84 target FALSE\n 8      1     1   -83 target FALSE\n 9      1     1   -82 target FALSE\n10      1     1   -81 target FALSE\n# … with 1,021,278 more rows\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\n\n\n\n\nOne thing that may have happened in the process above is that role is no longer a factor. So let’s convert it back before we finish.\n\npog_cts2 <- pog_cts %>%\n  mutate(role = fct_relevel(role, c(\"target\", \"critical\",\n                                    \"existing\", \"novel\", \"(blank)\")))\n\nNow let’s double check that the padding worked by looking again at some trials from subject 3.\n\n\n\n\n\nLooks good. Now let’s save all our hard work so that we can use pog_cts2 as a starting point for analysis.\n\nsaveRDS(pog_cts2, \"data-derived/pog_cts.rds\")"
  },
  {
    "objectID": "plotting.html",
    "href": "plotting.html",
    "title": "3  Plot probabilities",
    "section": "",
    "text": "In the last chapter, we completed data preprocessing and saved the resulting data to as an R binary RDS file, pog_cts.rds. In this chapter, we will import the data and use it to recreate some of the figures in Weighall et al. (2017).\nFirst, let’s load in {tidyverse} and then import the point-of-gaze data.\nAs usual, the first thing we should do is have a look at our data.\nThe data has sub_id and t_id which identify individual subjects and trials-within-subjects, respectively. But we are missing iformation about what group the subject belongs to (adult or child) and what experimental condition each trial belongs to."
  },
  {
    "objectID": "plotting.html#merge-eye-data-with-information-about-group-and-condition",
    "href": "plotting.html#merge-eye-data-with-information-about-group-and-condition",
    "title": "3  Plot probabilities",
    "section": "3.1 Merge eye data with information about group and condition",
    "text": "3.1 Merge eye data with information about group and condition\n\n3.1.1 Activity: Get trial condition\nThe first step is to create trial_cond, which has information about the group that each subject belongs to, the competitor type (existing or novel), and the condition (the identity of the critical object). The information we need is distributed across the subjects, trials, and stimuli tables (see Appendix A). Create trial_cond so that the resulting table matches the format below.\n\n\n# A tibble: 5,644 × 5\n   sub_id group  t_id ctype crit           \n    <int> <chr> <int> <chr> <chr>          \n 1      1 adult     1 novel competitor-day2\n 2      1 adult     2 novel competitor-day1\n 3      1 adult     3 exist competitor     \n 4      1 adult     4 exist competitor     \n 5      1 adult     6 novel untrained      \n 6      1 adult     7 novel competitor-day1\n 7      1 adult    11 novel untrained      \n 8      1 adult    13 novel competitor-day2\n 9      1 adult    14 novel untrained      \n10      1 adult    16 novel untrained      \n# … with 5,634 more rows\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\ntrials <- read_csv(\"data-raw/trials.csv\",\n                   col_types = \"iiiiii\")\n\nstimuli <- read_csv(\"data-raw/stimuli.csv\",\n                    col_types = \"iiciccc\")\n\nsubjects <- read_csv(\"data-raw/subjects.csv\",\n                     col_types = \"ic\")\n\ntrial_cond <- trials %>%\n  inner_join(stimuli, \"iv_id\") %>%\n  inner_join(subjects, \"sub_id\") %>%\n  select(sub_id, group, t_id, ctype, crit)"
  },
  {
    "objectID": "plotting.html#plot-probabilities-for-existing-competitors",
    "href": "plotting.html#plot-probabilities-for-existing-competitors",
    "title": "3  Plot probabilities",
    "section": "3.2 Plot probabilities for existing competitors",
    "text": "3.2 Plot probabilities for existing competitors\nWe want to determine the probability of looking at each image type at each frame in each condition. We will do this first for the existing competitors. Note there were two conditions here, indexed by crit: competitor and unrelated, corresponding to whether the critical image was a competitor or an unrelated item.\n\n3.2.1 Activity: Probs for exist condition\nFrom trial_cond, include only those trials where ctype takes on the value exist, combine with pog_cts, and then count the number of frames in each region for every combination of the levels of group (adult, child) and crit (competitor, unrelated). The resulting table should have the format below, where Y is the number of frames for each combination. While you’re at it, convert f_c to milliseconds (1000 * f_c / 60). Call the resulting table count_exist.\n\n\n\n\n\n\nHint: Counting things\n\n\n\n\n\nUse the count() function from {dplyr}. Take note of the .drop argument to deal with possible situations where there are zero observations. For example:\n\npets <- tibble(animal = factor(rep(c(\"dog\", \"cat\", \"ferret\"), c(3, 2, 0)),\n                               levels = c(\"dog\", \"cat\", \"ferret\")))\n\npets\n\n# A tibble: 5 × 1\n  animal\n  <fct> \n1 dog   \n2 dog   \n3 dog   \n4 cat   \n5 cat   \n\npets %>%\n  count(animal)\n\n# A tibble: 2 × 2\n  animal     n\n  <fct>  <int>\n1 dog        3\n2 cat        2\n\npets %>%\n  count(animal, .drop = FALSE)\n\n# A tibble: 3 × 2\n  animal     n\n  <fct>  <int>\n1 dog        3\n2 cat        2\n3 ferret     0\n\n\n\n\n\n\n\n# A tibble: 3,620 × 6\n   group crit         f_c role         Y     ms\n   <chr> <chr>      <int> <fct>    <int>  <dbl>\n 1 adult competitor   -90 target      54 -1500 \n 2 adult competitor   -90 critical    55 -1500 \n 3 adult competitor   -90 existing    55 -1500 \n 4 adult competitor   -90 novel       75 -1500 \n 5 adult competitor   -90 (blank)    181 -1500 \n 6 adult competitor   -89 target      59 -1483.\n 7 adult competitor   -89 critical    60 -1483.\n 8 adult competitor   -89 existing    58 -1483.\n 9 adult competitor   -89 novel       74 -1483.\n10 adult competitor   -89 (blank)    169 -1483.\n# … with 3,610 more rows\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\ncount_exist <- trial_cond %>%\n  filter(ctype == \"exist\") %>%\n  inner_join(pog_cts, c(\"sub_id\", \"t_id\")) %>%\n  count(group, crit, f_c, role, name = \"Y\", .drop = FALSE) %>%\n  mutate(ms = 1000 * f_c / 60)\n\n\n\n\nTo calculate the probability for each value of role, we need to calculate the number of opportunities for each combination of group, crit, and f_c, storing this in N. We do this using a windowed mutate, grouping the data before adding N for each group. We can then calculate the probability as p = Y / N.\n\nprob_exist <- count_exist %>%\n  group_by(group, crit, f_c) %>%\n  mutate(N = sum(Y), p = Y / N) %>%\n  ungroup()\n\nNow we are ready to plot.\n\nggplot(prob_exist %>% filter(role != \"(blank)\"), \n       aes(ms, p, color = role)) +\n  geom_line() +\n  facet_wrap(group ~ crit, nrow = 2)  +\n  coord_cartesian(xlim = c(-200, 1000))\n\n\n\n\n\n\n\n\nWeighall, AR, Lisa-Marie Henderson, DJ Barr, Scott Ashley Cairney, and Mark Gareth Gaskell. 2017. “Eye-Tracking the Time-Course of Novel Word Learning and Lexical Competition in Adults and Children.” Brain and Language 167: 13–27."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Weighall, AR, Lisa-Marie Henderson, DJ Barr, Scott Ashley Cairney, and\nMark Gareth Gaskell. 2017. “Eye-Tracking the Time-Course of Novel\nWord Learning and Lexical Competition in Adults and Children.”\nBrain and Language 167: 13–27."
  }
]